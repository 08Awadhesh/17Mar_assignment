{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063154d0-d511-4a88-8332-3caf833914db",
   "metadata": {},
   "source": [
    "**Q1:** What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "**Answer**:\n",
    "Missing values in a dataset refer to the absence of a specific value or information for one or more variables in a particular observation. They can occur due to various reasons, such as data entry errors, equipment malfunctions, or participant non-response in surveys.\n",
    "\n",
    "**Handling missing values is crucial for several reasons:**\n",
    "\n",
    "**(I) Biased or incomplete analysis**: Missing values can lead to biased or incomplete analysis as they can introduce errors and distort the statistical properties of the dataset. Ignoring missing values or simply removing observations with missing values can result in misleading conclusions.\n",
    "\n",
    "**(II) Distorted relationships:** Missing values can affect the relationships and correlations between variables. By excluding observations with missing values, we may inadvertently alter the structure and patterns of the data, leading to inaccurate results.\n",
    "\n",
    "**(III) Reduced sample size:** If missing values are not handled properly, it may result in a reduced sample size, limiting the power and generalizability of statistical analyses.\n",
    "\n",
    "**Impact on machine learning algorithms:** Many machine learning algorithms cannot directly handle missing values, and they require complete datasets for training. Therefore, addressing missing values becomes essential to ensure accurate model training and predictions.\n",
    "\n",
    "While numerous algorithms are affected by missing values, there are some that can handle them effectively. These include:\n",
    "\n",
    "**(I) Decision trees:** Decision trees can naturally handle missing values by creating additional branches in the tree based on available features. They split the data into different paths according to the available values.\n",
    "\n",
    "**(II) Random Forests:** Random Forests can handle missing values by averaging predictions from multiple decision trees. Each tree is built using a random subset of features, which reduces the impact of missing values on the overall prediction.\n",
    "\n",
    "**(III) Gradient Boosting methods**: Algorithms like Gradient Boosting (e.g., XGBoost, LightGBM) have built-in mechanisms to handle missing values. They can learn the best imputation strategy during the training process by optimizing the loss function.\n",
    "\n",
    "**(IV)Bayesian methods**: Bayesian algorithms, such as Bayesian Networks and Markov Chain Monte Carlo (MCMC), can handle missing values by incorporating uncertainty in the estimation process. They impute missing values based on the available data and their probabilistic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a6cd2-dd88-4ab9-80c9-f3472936c800",
   "metadata": {},
   "source": [
    "**Q2**: List down techniques used to handle missing data.  Give an example of each with python code.\n",
    "\n",
    "**Answer**:Here are four commonly used techniques along with an example of each using Python code:\n",
    "\n",
    "**(I) Deletion:** In this approach, the observations or variables with missing values are removed from the dataset. There are two sub-categories under deletion:\n",
    "\n",
    "a. Listwise deletion: Removes entire observations with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536db33b-b98e-4e72-b12d-e37cd628cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5],\n",
    "                   'B': [6, 7, 8, None, 10]})\n",
    "df_clean = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a063b17-1652-4072-8608-000a2c2020c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B\n",
       "0  1.0   6.0\n",
       "1  2.0   7.0\n",
       "2  NaN   8.0\n",
       "3  4.0   NaN\n",
       "4  5.0  10.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30598fba-2e6a-468f-88a5-9a0a144c6b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B\n",
       "0  1.0   6.0\n",
       "1  2.0   7.0\n",
       "4  5.0  10.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303aaf4c-c0d3-484b-8e42-232376e90b3b",
   "metadata": {},
   "source": [
    "**(II) Pairwise deletion:** Retains the observations for specific analyses by excluding missing values only for the variables involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e7c9e1-e8a8-480b-b262-d0980db38e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pairwise deletion\n",
    "df_clean_pairwise = df[['A', 'B']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c2caed-1b65-48bd-989c-7c76d9e84b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B\n",
       "0  1.0   6.0\n",
       "1  2.0   7.0\n",
       "4  5.0  10.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d962a2-772a-4674-a114-db6292c2dade",
   "metadata": {},
   "source": [
    "**(III) Mean/Mode/Median Imputation**: In this method, missing values are replaced with the mean, mode, or median value of the corresponding variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ac5fbc-6d57-4d44-8a74-b7c74424b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5],\n",
    "                   'B': [6, 7, 8, None, 10]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce5bbda-b47d-4f91-805e-8cb9c034542d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B\n",
       "0  1.0   6.0\n",
       "1  2.0   7.0\n",
       "2  NaN   8.0\n",
       "3  4.0   NaN\n",
       "4  5.0  10.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81028de0-bf97-4dd0-8329-4bf008041aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A_imputed']=df['A'].fillna(df['A'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13297e84-ec94-4272-a0d7-ea1007dbd86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_imputed</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_imputed    A\n",
       "0        1.0  1.0\n",
       "1        2.0  2.0\n",
       "2        3.0  NaN\n",
       "3        4.0  4.0\n",
       "4        5.0  5.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['A_imputed','A']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e05760f3-b106-48e0-b7d3-4d8f0008931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A_imputed']=df['A'].fillna(df['A'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b26e9385-d314-4bbb-82a4-e0a5d2fb8785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_imputed</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_imputed    A\n",
       "0        1.0  1.0\n",
       "1        2.0  2.0\n",
       "2        3.0  NaN\n",
       "3        4.0  4.0\n",
       "4        5.0  5.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['A_imputed','A']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4fecc58-8009-475c-b907-5b0213b7585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A_imputed']=df['A'].fillna(df['A'].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09186049-3c02-450e-b752-170c690091a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_imputed</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_imputed    A\n",
       "0        1.0  1.0\n",
       "1        2.0  2.0\n",
       "2        4.0  NaN\n",
       "3        4.0  4.0\n",
       "4        5.0  5.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['A_imputed','A']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a486e9-f888-4945-86b5-1471b3f9a72b",
   "metadata": {},
   "source": [
    "**(III) Interpolation:** Interpolation estimates missing values by considering the relationship between variables and the order of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1772f8f1-7ee9-426e-80b3-278773c1dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5],\n",
    "                   'B': [6, None, 8, 9, 10]})\n",
    "\n",
    "# Perform linear interpolation\n",
    "df_interpolated = df.interpolate(method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b8489b7-556d-472f-9aa2-5c9090743e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B\n",
       "0  1.0   6.0\n",
       "1  2.0   7.0\n",
       "2  3.0   8.0\n",
       "3  4.0   9.0\n",
       "4  5.0  10.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084a9a3-1210-4018-863f-9b67bba5d366",
   "metadata": {},
   "source": [
    "**(IV) Model-based Imputation:** This technique involves using statistical models to estimate missing values based on the relationship between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d30dde99-6049-4594-8f4c-3e8686221dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5],\n",
    "                   'B': [6, None, 8, 9, 10]})\n",
    "\n",
    "# Perform model-based imputation using IterativeImputer\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "df_imputed_model = imputer.fit_transform(df)\n",
    "df_imputed_model = pd.DataFrame(df_imputed_model, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73fae6a0-43a7-472e-8095-a56ba53cc90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.999873</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A          B\n",
       "0  1.000000   6.000000\n",
       "1  2.000000   7.000036\n",
       "2  2.999873   8.000000\n",
       "3  4.000000   9.000000\n",
       "4  5.000000  10.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imputed_model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd34e4-2936-46aa-98a2-72ac3d7ff04f",
   "metadata": {},
   "source": [
    "**Q3**: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "**Answer**:\n",
    "Imbalanced data refers to a situation where the distribution of classes in a classification problem is heavily skewed, with one class having a significantly larger number of instances than the other(s). In other words, there is a severe imbalance in the class distribution.\n",
    "\n",
    "When imbalanced data is not handled properly, it can lead to various challenges and negative consequences, including:\n",
    "\n",
    "**(I) Biased model performance:** The predictive models trained on imbalanced data tend to be biased towards the majority class. Since the majority class has more instances, the model may become overly focused on predicting that class accurately while ignoring the minority class. As a result, the model's performance in accurately predicting the minority class can be significantly compromised.\n",
    "\n",
    "**(II) Poor generalization**: Imbalanced data can negatively impact the generalization ability of a model. If the model is trained on imbalanced data, it may struggle to perform well on unseen data or real-world scenarios where class imbalance is prevalent. The model's lack of exposure to minority class instances during training can result in poor performance in classifying those instances during deployment.\n",
    "\n",
    "**(III) Misleading evaluation metrics**: Common evaluation metrics like accuracy can be misleading when dealing with imbalanced data. A model that simply predicts the majority class for all instances can achieve high accuracy, but it fails to capture the true performance and effectiveness of the model in identifying the minority class. Therefore, relying solely on accuracy can lead to incorrect assessments of the model's performance.\n",
    "\n",
    "**(IV) Increased false negatives or false positives**: Depending on the application, imbalanced data can cause the model to produce an excessive number of false negatives or false positives. For example, in a medical diagnosis scenario, if the minority class represents a rare disease, a model trained on imbalanced data may incorrectly classify most instances as negative, leading to a high number of false negatives and missing potential cases of the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39551a02-3454-47df-923c-c33fd185dce5",
   "metadata": {},
   "source": [
    "**Q4**: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required.\n",
    "\n",
    "**Answer**:\n",
    "Upsampling and downsampling are techniques used to address class imbalance in a dataset by modifying the class distribution. Here's an explanation of each technique along with an example scenario where they are required:\n",
    "\n",
    "**Upsampling (or oversampling)**: Upsampling involves randomly duplicating instances from the minority class to increase its representation in the dataset. This is done until a more balanced class distribution is achieved. The goal is to provide the model with more examples of the minority class, helping it learn its distinguishing characteristics better.\n",
    "\n",
    "**Example**: Consider a credit card fraud detection dataset where only 2% of the transactions are fraudulent (minority class). To train a model that can accurately detect fraud, the dataset can be upsampled by randomly duplicating instances of fraudulent transactions until the fraud and non-fraud classes have a more balanced distribution.\n",
    "\n",
    "**Downsampling (or undersampling):** Downsampling involves randomly removing instances from the majority class to reduce its representation in the dataset. This is done until a more balanced class distribution is achieved. The aim is to reduce the dominance of the majority class, allowing the model to focus on learning the minority class patterns without being overwhelmed by the majority class.\n",
    "\n",
    "**Example:** In an email spam classification dataset, suppose 90% of the emails are non-spam (majority class) and only 10% are spam (minority class). Downsampling can be applied by randomly removing instances of non-spam emails until the spam and non-spam classes have a more balanced distribution. This helps the model avoid being biased towards classifying all emails as non-spam due to the heavy dominance of the majority class.\n",
    "\n",
    "**When to use upsampling and downsampling:**\n",
    "Upsampling is typically used when the minority class has limited instances, and the dataset imbalance is severe. By generating synthetic examples of the minority class, upsampling ensures better representation and enables the model to learn its characteristics effectively.\n",
    "\n",
    "Downsampling is suitable when the dataset is large, and the majority class overwhelms the minority class. By reducing the number of instances in the majority class, downsampling helps the model focus on the minority class and prevents it from being biased towards the dominant class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbdf80-880b-4915-922c-244bcf0683b6",
   "metadata": {},
   "source": [
    "**Q5:** What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "**Answer**: Data augmentation is a technique used to artificially increase the size of a dataset by creating new synthetic samples through various transformations or modifications of the existing data. It is commonly employed in machine learning and deep learning tasks, especially when the available dataset is limited.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique designed to address class imbalance in datasets. It focuses on increasing the representation of the minority class by generating synthetic samples that are similar to existing minority class instances.\n",
    "\n",
    "Here's an overview of how SMOTE works:\n",
    "\n",
    "(I) Identify minority class instances: In SMOTE, the minority class is the class with fewer instances compared to the majority class.\n",
    "\n",
    "(II) Select a minority class instance: Randomly choose an instance from the minority class.\n",
    "\n",
    "(III) Find its k nearest neighbors: Determine the k nearest neighbors of the selected instance based on a distance metric (e.g., Euclidean distance) in the feature space.\n",
    "\n",
    "(IV) Generate synthetic samples: For each selected instance, SMOTE creates synthetic samples by interpolating between the chosen instance and its k nearest neighbors. It selects a random number between 0 and 1, and then for each feature, calculates the difference between the feature values of the chosen instance and one of its neighbors. The synthetic sample is created by adding this difference multiplied by the random number to the chosen instance's feature values.\n",
    "\n",
    "(V) Repeat the process: Repeat steps (II) to (IV) to create the desired number of synthetic samples.\n",
    "\n",
    "By generating synthetic samples in this way, SMOTE effectively increases the number of instances in the minority class. It helps to balance the class distribution, allowing machine learning models to learn more effectively and make accurate predictions for the minority class.\n",
    "\n",
    "**SMOTE has several advantages over simple upsampling techniques like duplication:**\n",
    "SMOTE generates synthetic samples that are not mere duplicates of existing instances, introducing more variation into the minority class.\n",
    "It reduces the risk of overfitting that may occur with simple duplication, as SMOTE creates samples in the feature space rather than copying exact instances.\n",
    "SMOTE can be combined with other data augmentation techniques to further enhance the dataset and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede9da1-9612-4cb8-93b1-95a1989edd95",
   "metadata": {},
   "source": [
    "**Q6**: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "**Answer**:\n",
    "Outliers are observations or data points that significantly deviate from the majority of the data in a dataset. These are data points that lie far away from the central tendency of the data distribution. Outliers can occur due to various reasons, such as data entry errors, measurement errors, experimental anomalies, or genuine extreme values.\n",
    "\n",
    "**Handling outliers is essential for several reasons**:\n",
    "\n",
    "**(I) Distorted statistical measures:** Outliers can significantly affect statistical measures such as the mean and standard deviation. Since these measures are sensitive to extreme values, outliers can skew their values, leading to inaccurate representations of the data distribution.\n",
    "\n",
    "**(II) Biased analysis and modeling**: Outliers can distort the relationships and patterns between variables, leading to biased analysis and modeling. Models trained on datasets with outliers may produce unreliable predictions or inaccurate estimates, as they are influenced by the extreme values.\n",
    "\n",
    "**(III) Inaccurate insights and conclusions**: Outliers can misrepresent the true characteristics of the data and lead to incorrect insights and conclusions. Making decisions or drawing conclusions based on data that includes outliers can result in misguided actions or strategies.\n",
    "\n",
    "**(IV) Negative impact on machine learning algorithms**: Outliers can adversely affect the performance of machine learning algorithms. Models trained on outlier-influenced data may overfit to the extreme values, resulting in poor generalization and decreased predictive accuracy on unseen data.\n",
    "\n",
    "**(V) Violation of assumptions:** Outliers can violate the assumptions of certain statistical models and techniques. For example, linear regression assumes that the data follows a normal distribution, and outliers can violate this assumption, leading to unreliable regression coefficients and predictions.\n",
    "\n",
    "**To handle outliers, various techniques can be employed:**\n",
    "\n",
    "**(I) Winsorization or Trimming:** Winsorization involves replacing extreme values with less extreme values, typically by setting them to a specific percentile or a predetermined cutoff value. This helps mitigate the influence of outliers while preserving the data distribution.\n",
    "\n",
    "**(II) Z-score or Standard Deviation Method:** This method involves identifying data points that fall outside a specific threshold based on the standard deviation from the mean. Outliers beyond the threshold can be removed or replaced with more appropriate values.\n",
    "\n",
    "**(III) Robust statistical measures:** Instead of relying on mean and standard deviation, robust statistical measures such as median and interquartile range (IQR) can be used, as they are less affected by extreme values.\n",
    "\n",
    "**(IV) Machine learning algorithms**: Some machine learning algorithms are robust to outliers by design, such as decision trees and random forests. These algorithms can handle outliers to some extent and provide more reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c66b78-8d23-4e4a-9b0f-497929163f91",
   "metadata": {},
   "source": [
    "**Q7:** You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "**Answer:** When dealing with missing data in customer data analysis, several techniques can be used to handle the missing values. The choice of technique depends on the nature of the data, the extent of missingness, and the specific requirements of the analysis. Here are some commonly used techniques:\n",
    "\n",
    "**(I) Deletion:**\n",
    "\n",
    "Listwise deletion: Removing entire observations (rows) with missing values. This approach is suitable when the missing values are minimal and do not significantly affect the analysis.\n",
    "Pairwise deletion: Retaining observations for specific analyses by excluding missing values only for the variables involved. This approach preserves more data but can lead to incomplete observations for some analyses.\n",
    "\n",
    "**(II) Mean/Mode/Median Imputation:**\n",
    "Replacing missing values with the mean, mode, or median value of the corresponding variable. This approach is suitable when the missing values are assumed to be missing at random and do not introduce bias.\n",
    "\n",
    "**(III) Interpolation:**\n",
    "Using interpolation methods (e.g., linear interpolation) to estimate missing values based on the values of neighboring data points. This approach is useful when there is a temporal or spatial relationship in the data.\n",
    "**(IV) Model-based imputation**:\n",
    "Utilizing statistical models to predict missing values based on the available data. Techniques like regression, decision trees, or k-nearest neighbors can be employed to impute missing values.\n",
    "\n",
    "**(IV) Multiple imputation:**\n",
    "Generating multiple imputations by creating plausible values for missing data based on the observed data and its relationships. Multiple imputation takes into account the uncertainty associated with imputing missing values.\n",
    "\n",
    "**(V) Domain-specific knowledge:**\n",
    "Leveraging domain knowledge or expert insights to impute missing values. This can involve using external data sources, historical patterns, or business rules to estimate missing values.\n",
    "It is crucial to consider t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291bc08f-0835-418b-93d1-3113af0a6637",
   "metadata": {},
   "source": [
    "**Q8**: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "**Answer** When dealing with missing data in a large dataset, it is important to understand whether the missingness is random or if there is a pattern to it. Here are some strategies you can use to determine the nature of missing data:\n",
    "\n",
    "**(I) Statistical summary:** Calculate summary statistics, such as mean, median, or mode, for the variables with missing data and compare them to the corresponding statistics of the complete data. If the summary statistics are significantly different, it may indicate non-random missingness.\n",
    "\n",
    "**(II) Missing data visualization:** Create visualizations, such as histograms or bar plots, to compare the distribution of missing values across different variables. Look for patterns or correlations between missingness and other variables or attributes. For example, missingness may be concentrated in specific subgroups or occur at particular time points.\n",
    "\n",
    "**(III) Missing data mechanisms**: Familiarize yourself with different missing data mechanisms. There are three common mechanisms: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). MCAR implies that the missingness is independent of any observed or unobserved variables. MAR means that the missingness can be explained by other observed variables. MNAR indicates that the missingness depends on unobserved data or the variable itself.\n",
    "\n",
    "**(IV) Imputation and analysis**: Perform data imputation techniques to estimate missing values and then analyze the imputed data. Compare the results obtained from the imputed dataset with the complete dataset. If the missing data does not significantly affect the analysis results, it suggests that the missingness might be random.\n",
    "\n",
    "**(V) Missing data tests**: There are statistical tests specifically designed to assess the pattern of missing data. Examples include the Little's MCAR test, which tests the MCAR assumption, and the Missingness Pattern Test, which evaluates the relationship between missingness and other variables. These tests can help provide evidence for or against the random missingness assumption.\n",
    "\n",
    "**(VI) Expert knowledge:** Consult domain experts or individuals familiar with the data to gain insights into the missing data patterns. They might provide valuable information about the data collection process or any known reasons for missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883e6b0-fb01-46c8-ac90-5a5345d0261d",
   "metadata": {},
   "source": [
    "**Q9**: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "**Answer** When dealing with imbalanced datasets in a medical diagnosis project, where the majority of patients do not have the condition of interest, it is important to consider strategies to evaluate the performance of your machine learning model effectively. Here are some strategies you can use:\n",
    "\n",
    "**(I) Class distribution analysis:** Examine the class distribution in the dataset to understand the severity of the class imbalance. Calculate the percentage of positive and negative instances in the dataset. This analysis will help you understand the degree of class imbalance and the potential challenges associated with it.\n",
    "\n",
    "**(II) Evaluation metrics**: Traditional accuracy may not be an appropriate evaluation metric for imbalanced datasets since it can be misleading due to the disproportionate number of samples in each class. Instead, consider using evaluation metrics that are more suitable for imbalanced datasets, such as precision, recall, F1 score, area under the ROC curve (AUC-ROC), or area under the precision-recall curve (AUC-PR). These metrics provide a more comprehensive understanding of the model's performance.\n",
    "\n",
    "**(III) Resampling techniques**: Imbalanced datasets can benefit from resampling techniques that aim to balance the class distribution. Two common approaches are:\n",
    "\n",
    "**(IV) Oversampling**: Increase the number of instances in the minority class by replicating existing instances or generating synthetic samples using techniques like Synthetic Minority Over-sampling Technique (SMOTE).\n",
    "\n",
    "**(V) Undersampling:** Reduce the number of instances in the majority class by randomly removing samples or selecting a subset of instances. Care should be taken to ensure that the resulting dataset remains representative of the underlying data.\n",
    "\n",
    " It's important to note that resampling techniques should be applied carefully to avoid introducing bias or overfitting. Cross-validation or train-validation-test splits should be used appropriately.\n",
    "\n",
    "**(VI) Class weighting**: Many machine learning algorithms provide an option to assign different weights to different classes. By assigning higher weights to the minority class, you can encourage the model to pay more attention to correctly predicting the rare class.\n",
    "\n",
    "**(V) Ensemble methods**: Ensemble methods like bagging and boosting can be useful for imbalanced datasets. They combine multiple models or adapt the training process to give more importance to the minority class, improving overall performance.\n",
    "\n",
    "**(V) Anomaly detection**: If the class imbalance is extreme, you may consider treating the problem as an anomaly detection task. By considering the majority class as the normal class and the minority class as anomalies, you can use anomaly detection algorithms or techniques to identify and evaluate the performance of the model in detecting the condition of interest.\n",
    "\n",
    "**(VI) Domain-specific considerations:** In medical diagnosis projects, it's crucial to involve domain experts who can provide insights and guidance. They can help assess the model's performance in a clinical context and provide recommendations for selecting appropriate evaluation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659d966-9447-4476-94ce-0507d30f82a2",
   "metadata": {},
   "source": [
    "**Q10:** When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "**Answer**: When dealing with an unbalanced dataset in the context of estimating customer satisfaction, where the majority of customers report being satisfied, you can employ various methods to balance the dataset and down-sample the majority class. Here are some techniques you can use:\n",
    "\n",
    "**(I) Random undersampling**: Randomly remove instances from the majority class to match the number of instances in the minority class. This approach can be simple to implement, but it may discard potentially useful information.\n",
    "\n",
    "**(II) Cluster-based undersampling**: Use clustering algorithms to identify clusters within the majority class and then select representative instances from each cluster while discarding the rest. This approach aims to preserve the diversity of the majority class while reducing its size.\n",
    "\n",
    "**(III) Tomek links**: Identify pairs of instances from different classes that are nearest neighbors to each other, and remove the majority class instances from these pairs. This method helps in removing overlapping or borderline instances.\n",
    "\n",
    "**(IV) NearMiss algorithm:** This algorithm selects instances from the majority class based on their proximity to instances in the minority class. NearMiss variants, such as NearMiss-1, NearMiss-2, or NearMiss-3, apply different strategies to select instances.\n",
    "\n",
    "**(V)Edited Nearest Neighbors (ENN)**: Use the k-nearest neighbors algorithm to identify instances in the majority class that are misclassified and remove them. This approach focuses on reducing overlapping instances.\n",
    "\n",
    "**(VI) Synthetic Minority Over-sampling Technique (SMOTE)**: Generate synthetic instances in the minority class by interpolating between existing instances. SMOTE can help increase the number of minority class samples while maintaining the distribution and patterns of the original data.\n",
    "\n",
    "**(VII) Combination of oversampling and undersampling:** Apply a combination of oversampling techniques on the minority class and undersampling techniques on the majority class to balance the dataset. This can be done, for example, by oversampling the minority class using SMOTE and then randomly undersampling the majority class.\n",
    "\n",
    "It's important to note that downsampling the majority class may result in the loss of valuable information. Therefore, it's advisable to evaluate the performance of the model using different sampling techniques and compare the results to determine the most suitable approach for your specific dataset and problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b23d-fe32-46ae-8744-cfd7b5b1fb46",
   "metadata": {},
   "source": [
    "**Q11**: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "**Answer**:\n",
    "When working with a dataset that is unbalanced and contains a low percentage of occurrences for a rare event, you can employ various methods to balance the dataset and up-sample the minority class. Here are some techniques you can use:\n",
    "\n",
    "**(I) Random oversampling:** Randomly duplicate instances from the minority class to increase its size. This approach can introduce redundancy in the dataset, but it is simple to implement.\n",
    "\n",
    "**(II) SMOTE (Synthetic Minority Over-sampling Technique)**: Generate synthetic instances in the minority class by interpolating between existing instances. SMOTE creates new samples by considering the feature space between neighboring minority class instances. This method helps increase the number of minority class samples while maintaining the distribution and patterns of the original data.\n",
    "\n",
    "**(III) ADASYN (Adaptive Synthetic Sampling)**: Similar to SMOTE, ADASYN also generates synthetic instances but focuses on the regions with fewer examples. ADASYN assigns different weights to the minority class instances based on their difficulty in learning. It gives more attention to the minority class instances that are harder to learn.\n",
    "\n",
    "**(IV) SMOTE-ENN**: Combine SMOTE with the Edited Nearest Neighbors (ENN) undersampling technique. SMOTE is applied first to oversample the minority class, and then ENN is used to remove any instances from both classes that are misclassified by a k-nearest neighbors classifier. This method aims to improve the quality of the synthetic samples generated by SMOTE.\n",
    "\n",
    "**(V) SMOTE-Tomek**: Combine SMOTE with the Tomek links undersampling technique. SMOTE is used to oversample the minority class, and then Tomek links are used to identify and remove pairs of instances from different classes that are nearest neighbors to each other. This method focuses on removing overlapping or borderline instances.\n",
    "\n",
    "**(VI) Cluster-based oversampling**: Apply clustering algorithms to identify clusters within the minority class and then generate synthetic instances within each cluster to increase the minority class representation. This approach helps to preserve the diversity and distribution within the minority class.\n",
    "\n",
    "**(VII) Ensemble methods:** Utilize ensemble methods, such as bagging or boosting, specifically designed for imbalanced datasets. These methods combine multiple models or adapt the training process to give more importance to the minority class, thereby improving overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45072cb-1fb8-4781-bd15-af7290738d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
